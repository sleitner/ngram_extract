{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stage 0: Load up modules\n",
    "# sc is availabile\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "import re\n",
    "import shutil # because dbutils doesn't expose it's methods all the time?\n",
    "import unicodedata\n",
    "from operator import add\n",
    "from functools import partial\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.feature import Word2Vec\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from functools import partial\n",
    "from itertools import chain, izip, tee\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "try:\n",
    "  STOP_WORDS = stopwords.words(\"english\") # assumes stripped punctucation ([its, ain, t]), a set\n",
    "except LookupError:\n",
    "  nltk.download(\"stopwords\") \n",
    "  STOP_WORDS = stopwords.words(\"english\")\n",
    "\n",
    "class SkillsRawAccess(object):\n",
    "    \"\"\"Simple access API to raw skills data on S3\"\"\"\n",
    "    def __init__(self, bucket='dssg-labor', base='mnt', parent=''):\n",
    "        self.bucket = bucket\n",
    "        self.base = base\n",
    "        self.parent = parent\n",
    "        self.format = 'com.databricks.spark.csv'\n",
    "      \n",
    "    def rootpath(self, *args):\n",
    "        return os.path.join(self, '/', self.base, self.bucket, self.parent, *args)\n",
    "\n",
    "    def writepath(self, file_name):\n",
    "        return self.rootpath(file_name)\n",
    "      \n",
    "    def readpath(self, file_name):\n",
    "        return self.rootpath(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# snl #####################################################\n",
    "######################################################\n",
    "######################################################\n",
    "######################################################\n",
    "# ... write out labeled versions of the raw corpora\n",
    "S = SkillsRawAccess(bucket='skills-public', parent='usajobs')\n",
    "\n",
    "name = 'raw_corpora_a'\n",
    "S.bucket = 'skills-public'\n",
    "S.parent = 'machine_learning' + '/preprocess/'\n",
    "copora_nlp_a = sqlContext.read\\\n",
    "                         .format(S.format)           \\\n",
    "                         .options(delimiter='\\t')    \\\n",
    "                         .options(header='true')     \\\n",
    "                         .load(S.writepath(name + '.tsv'))\n",
    "\n",
    "# snl now I have a processed corpus?\n",
    "\n",
    "name = 'skills_master_table'\n",
    "S.bucket = 'skills-public'\n",
    "S.parent = 'machine_learning' + '/table/'\n",
    "unique_skills = pd.read_csv('/dbfs' + S.writepath(name + '.tsv'), sep='\\t')\\\n",
    "                  .drop_duplicates('Element ID')\n",
    "  \n",
    "# snl now I have sklls table?\n",
    "  \n",
    "skill_uuid_ = unique_skills.set_index(nlp.transforms[0])['skill_uuid']\\\n",
    "                          .to_dict()\n",
    "nlp_a_skill_uuid = sc.broadcast(skill_uuid_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print copora_nlp_a.describe()\n",
    "#print copora_nlp_a.first().document\n",
    "df = copora_nlp_a.sample(False, 0.00005, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# for future: \n",
    "#  - N-Grams never cross periods or commas\n",
    "#  - break N-grams on stop words?\n",
    "def ngram_creator(doc, N):\n",
    "    join_underscore = \"_\".join\n",
    "    for start_idx in xrange(len(doc)-N+1):\n",
    "      yield join_underscore(doc[start_idx : start_idx+N])\n",
    "\n",
    "def ngram_counter(doc, N):\n",
    "    return Counter(list(ngram_creator(doc.split(),N)))\n",
    "  \n",
    "def sum_histograms(counter_list): \n",
    "  cnt = Counter()\n",
    "  for c in counter_list:\n",
    "    cnt += c\n",
    "  return cnt\n",
    "\n",
    "\n",
    "#check the pmi count formula\n",
    "import math\n",
    "def pmi(phrase, ngram_hist, unigram_hist, sep='_'):\n",
    "  min_unigrams = 3\n",
    "  #extra count for every p in the denominator\n",
    "  n = len(phrase.split(sep)) \n",
    "  word_count = sum(unigram_hist.values())\n",
    "  norm = 0\n",
    "  nwcount = 0\n",
    "  for w in phrase.split(sep):\n",
    "    norm += math.log(unigram_hist[w])\n",
    "    nwcount += unigram_hist[w]\n",
    "  pmi = math.log(ngram_hist[phrase]) + math.log((n-1)*word_count) - norm\n",
    "  # there should be at least 3 of each unigram on average\n",
    "  if nwcount/n < min_unigrams:\n",
    "    pmi = -100\n",
    "  return pmi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimize (the collect?)\n",
    "unigram_hist = sum_histograms(df.map(lambda x: ngram_counter(x.document, 1)).collect()) \n",
    "bigram_hist = sum_histograms(df.map(lambda x: ngram_counter(x.document, 2)).collect()) \n",
    "#trigram_hist = sum_histograms(df.map(lambda x: ngram_counter(x.document, 3)).collect()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy \n",
    "bigram_pmi = copy.deepcopy(bigram_hist)\n",
    "for phrase in bigram_hist:\n",
    "  bigram_pmi[phrase] = pmi(phrase, bigram_hist, unigram_hist)\n",
    "\n",
    "print '-----------------pmi'\n",
    "for word, count in bigram_pmi.most_common(10):\n",
    "    print word, count\n",
    "print '-----------------collocation counts'\n",
    "for word, count in bigram_hist.most_common(10):\n",
    "    print word, count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# after choosing a cutoff, construct a list and remove the known skills from the list\n",
    "# use negative md5 numbers as values in a dict of those words\n",
    "# run the same labeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... using this labeler function\n",
    "def labeler(document, skill_uuid):\n",
    "  def label(document, skill_uuid):  \n",
    "    join_spaces = \" \".join # for runtime efficiency\n",
    "\n",
    "    N = 5  \n",
    "    doc = document.split()\n",
    "    doc_len = len(doc)\n",
    "\n",
    "    start_idx = 0\n",
    "\n",
    "    # Yield a generator of document skills/non skills that advances\n",
    "    # index pointer (`offset`) to end of skill or current non skill\n",
    "    # while yielding that skill uuid or non skill token (controlled by\n",
    "    # `found_skill` flag) to the callee so they can do whatever with it\n",
    "    while start_idx < doc_len:\n",
    "      found_skill = False\n",
    "      offset = 1\n",
    "\n",
    "      lookahead = min( N, doc_len - start_idx )\n",
    "      for idx in range( lookahead, 0, -1):\n",
    "        ngram = join_spaces(doc[start_idx : start_idx+idx])\n",
    "        if ngram in skill_uuid:\n",
    "          found_skill = True\n",
    "          offset = idx\n",
    "\n",
    "          yield skill_uuid[ngram]\n",
    "          break\n",
    "\n",
    "      if not found_skill:\n",
    "        yield doc[start_idx]\n",
    "\n",
    "      start_idx += offset\n",
    "      \n",
    "  return ' '.join(list(label(document, skill_uuid)))\n",
    "\n",
    "## doc test type debug code/stuff\n",
    "skill_uuid = {'computer programming':'23423423', 'fruit':'343'}\n",
    "doc = \"computer programming fruit You fruit   should be familar with computer programming fruit a\"\n",
    "print labeler(doc, skill_uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_corpora_nlp_a = copora_nlp_a.map(lambda x: labeler(x.document, nlp_a_skill_uuid.value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'sam_new_labeled_corpora_a'\n",
    "S.bucket = 'skills-public'\n",
    "S.parent = 'machine_learning' + '/preprocess/'\n",
    "S.writepath(name + '.tsv')\n",
    "\n",
    "labeled_corpora_nlp_a = copora_nlp_a.map(lambda x: labeler(x.document, nlp_a_skill_uuid.value))\n",
    "labeled_corpora_nlp_a.map(Row('document'))\\\n",
    "                     .toDF()\\\n",
    "                     .write\\\n",
    "                     .format(S.format)           \\\n",
    "                     .mode('overwrite')          \\\n",
    "                     .options(delimiter='\\t')    \\\n",
    "                     .options(header='true')     \\\n",
    "                     .save(S.writepath(name + '.tsv'))\n",
    "######################################################\n",
    "######################################################\n",
    "######################################################\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... write out master vocabulary file\n",
    "# note: update, want skill ngram, context (3 or 4 sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "name": "pipeline_to_nngrams",
  "notebookId": 19884
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
